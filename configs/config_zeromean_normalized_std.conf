# Zero-Mean Normalization + Unit Variance Normalization + Std Aggregation configuration
# New modular data processing format

[training_data]
# Unique identifier for this configuration (raises error if name exists with different hash)
unique_name = zeromean_normalized_std

# Path to the training dataset (GPKG file)
paths = /mnt/ssddata/PbNNMod/label_preparation/classification/crop_classification/cldef_5/Niedersachsen_2022/Niedersachsen_2022_InvekosDataset_c823cf0c.gpkg,/mnt/ssddata/PbNNMod/label_preparation/classification/crop_classification/cldef_5/NRW_2022/NRW_2022_InvekosDataset_*.gpkg

# Column name containing the classification IDs
column_id = cora_id

# Comma-separated list of classification classes to use
classes = 1,4,6,12

# Balance class distribution by subsampling to minimum class count
equal_class_dist = true

# Randomly shuffle patch labels after loading and balancing (for data leak testing and random baseline)
shuffle_labels = false

[data_processing]
# zero-mean normalization + unit variance normalization + std aggregation
# Applied in order: shuffle → zero_mean → normalize → quantiles → aggregation
shuffled = false
zero_mean = true
normalized = true
quantiles = false
aggregation = std

[satellite_data]
# Sentinel-1 orbit identifiers (comma-separated)
orbits = D139

# Dates to process (YYYYMMDD format, comma-separated)
dates = 20220611

# File pattern for satellite data with {orbit} placeholder
file_pattern = /mnt/cephfs/data/CorDAu/S1/download/preproc_1/data/UTMZ_32N/2022/{orbit}/*.tif

[processing]
# Number of worker processes for data loading
num_workers = 4

# Maximum memory usage in MB
max_memory_mb = 2048

# Output format for processed data
output_format = npz

[patch_extraction]
# Number of patches to extract per feature
n_patches_per_feature = 50

# Area ratio factor for patch extraction
n_patches_per_area = 1.0

[neural_network]
# 10*10 pixel input patches - architecture depends on aggregation setting
patch_size = 10
batch_size = 32
network_architecture_id = linear_stats_net
dropout_rate = 0.2
activation_function = relu
optimizer = adam
layer_sizes = 64,32,16
n_epochs = 50
early_stopping_patience = 10
